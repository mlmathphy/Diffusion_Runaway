import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import os
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from scipy.io import savemat

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print('Device:', device)

root_data = 'data/'
root_output = 'output/'
torch.cuda.empty_cache()
# Set random seed
np.random.seed(12345)
torch.manual_seed(12345)

# --- Load your data from the 3D SDE simulation ---
dim = 3  # (p, xi, r)

# Load the datasets generated by our modified 3D function
print("Loading datasets...")
exit_xx = np.load(os.path.join(root_data, 'exit_x.npy'))
exit_delta = np.load(os.path.join(root_data, 'exit_delta.npy'))

exit_x = exit_xx.copy()
exit_x[:, 0] = exit_xx[:, 0] * exit_xx[:, 1]
exit_x[:, 1] = exit_xx[:, 0] * np.sqrt(1 - exit_xx[:, 1]**2)

print(f"Original dataset sizes:")
print(f"exit_x shape: {exit_x.shape}")
print(f"exit_delta shape: {exit_delta.shape}")
print(f"Exit rate: {100 * np.mean(exit_delta == 0):.2f}%")





# Find the deterministic boundary
print("\nAnalyzing deterministic boundary...")
exit_indices = np.where((exit_xx[:, 2] > 0.8) & (exit_delta == 0))[0]  # Where exit happens
stay_indices = np.where((exit_xx[:, 2] > 0.8) & (exit_delta == 1))[0]  # Where particle stays


print(exit_indices.shape)
print(stay_indices.shape)


if len(exit_indices) > 0:
    min_exit_r = np.min(exit_xx[exit_indices, 2])
    print(f"Minimum r value where exit occurs: {min_exit_r:.6f}")

    print("âœ“ Confirmed: All points below threshold always stay (exit_delta=1)")
    deterministic_threshold = min_exit_r
    
    # Filter training data to only include uncertain region
    uncertain_mask = exit_xx[:, 2] >= deterministic_threshold
    print(f"Training on {np.sum(uncertain_mask)}/{len(exit_xx)} points ({100*np.sum(uncertain_mask)/len(exit_xx):.1f}%)")
    
    # Apply filter
    exit_x_filtered = exit_x[uncertain_mask]
    exit_delta_filtered = exit_delta[uncertain_mask]
    exit_xx_filtered = exit_xx[uncertain_mask]
    
    print(f"Filtered dataset - Exit rate: {100 * np.mean(exit_delta_filtered == 0):.2f}%")
    
    # Save threshold for prediction
    threshold_info = {
        'deterministic_threshold': deterministic_threshold,
        'below_threshold_value': 1.0  # Always predict 1 (stay) below threshold
    }
    np.save(os.path.join(root_data, 'deterministic_threshold.npy'), threshold_info)
    
    # Use filtered data
    exit_x = exit_x_filtered
    exit_delta = exit_delta_filtered
    exit_xx = exit_xx_filtered
        
else:
    print("No exits found in dataset")
    deterministic_threshold = None

# Subsample from large dataset to make training manageable
total_samples = exit_x.shape[0]
max_samples = min(2000000, total_samples)  # Use up to 500k samples
sample_indices = np.random.choice(total_samples, size=max_samples, replace=False)

X = exit_x[sample_indices]  # 3D positions
Y = exit_delta[sample_indices]  # Binary exit flags (0=exit, 1=stay)

print(f"Sampled dataset shapes: X={X.shape}, Y={Y.shape}")
print(f"Sampled exit rate: {100 * np.mean(Y == 0):.2f}%")

# Save for inspection
mdic = {"loc": X, "prob": Y}
savemat(os.path.join(root_output, "escape_prob.mat"), mdic)

# Normalize features - important for 3D physics data
mu_weight = X.mean(axis=0)
s_weight = X.std(axis=0)

# Save normalization parameters
os.makedirs(root_data, exist_ok=True)
with open(os.path.join(root_data, 'weight_mean_std.npy'), 'wb') as f:
    np.save(f, mu_weight)
    np.save(f, s_weight)

print(f"Normalization - Mean: {mu_weight}, Std: {s_weight}")

# Apply normalization
X_normalized = (X - mu_weight) / s_weight

# Convert to PyTorch tensors
X_tensor = torch.tensor(X_normalized, dtype=torch.float32)
Y_tensor = torch.tensor(Y.reshape(-1, 1), dtype=torch.float32)

print(f"Tensor shapes: X={X_tensor.shape}, Y={Y_tensor.shape}")

# Train/Validation split
X_train, X_val, Y_train, Y_val = train_test_split(
    X_tensor, Y_tensor, test_size=0.01, random_state=123, stratify=Y
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Validation set: {X_val.shape[0]} samples")

# Create datasets and data loaders
train_dataset = TensorDataset(X_train, Y_train)
val_dataset = TensorDataset(X_val, Y_val)

# Use larger batch size for efficiency with large dataset
batch_size = int(total_samples/2)  # Increased from 4096
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2)

print(f"Number of training batches: {len(train_loader)}")
print(f"Number of validation batches: {len(val_loader)}")

def make_folder(folder):
    if not os.path.exists(folder):
        os.makedirs(folder)

def save_model(model):
    make_folder(root_output)
    filename = 'NN_3D_escape_model'
    checkpoint = {
        'state_dict': model.state_dict(),
        'normalization': {'mean': mu_weight, 'std': s_weight},
        'dim': dim
    }
    torch.save(checkpoint, os.path.join(root_output, filename + '.pt'))
    print(f"Model saved to {os.path.join(root_output, filename + '.pt')}")

# Define deeper model with dropout for 3D physics problem
class EscapeModel(nn.Module):
    # def __init__(self):
    #     super(EscapeModel, self).__init__()
    #     self.dim = dim
    #     self.hid_size = 512  # Increased hidden size for complex 3D physics
    #     self.net = nn.Sequential(
    #         nn.Linear(self.dim, self.hid_size),
    #         nn.LeakyReLU(0.01),
    #         nn.BatchNorm1d(self.hid_size),
    #         nn.Dropout(0.3),
            
    #         nn.Linear(self.hid_size, self.hid_size),
    #         nn.LeakyReLU(0.01),
    #         nn.BatchNorm1d(self.hid_size),
    #         nn.Dropout(0.3),
            
    #         nn.Linear(self.hid_size, self.hid_size//2),
    #         nn.LeakyReLU(0.01),
    #         nn.BatchNorm1d(self.hid_size//2),
    #         nn.Dropout(0.2),
            
    #         nn.Linear(self.hid_size//2, self.hid_size//4),
    #         nn.LeakyReLU(0.01),
    #         nn.Dropout(0.1),
            
    #         nn.Linear(self.hid_size//4, 1),
    #         nn.Sigmoid()
    #     )
    def __init__(self):
        super(EscapeModel, self).__init__()
        self.dim = dim
        self.hid_size = 256
        self.net = nn.Sequential(
            nn.Linear(self.dim, self.hid_size),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.2),
            nn.Linear(self.hid_size, self.hid_size),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.2),
            nn.Linear(self.hid_size, self.hid_size),
            nn.LeakyReLU(0.01),
            nn.Dropout(0.2),
            nn.Linear(self.hid_size, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return self.net(x)

model = EscapeModel().to(device)
print(f"Model has {sum(p.numel() for p in model.parameters())} parameters")

# Binary cross entropy loss with class weights for imbalanced data
# pos_weight = torch.tensor([np.mean(Y == 0) / np.mean(Y == 1)]).to(device)
# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)

# Remove sigmoid from model since BCEWithLogitsLoss includes it
# model.net[-1] = nn.Identity()  # Replace sigmoid with identity

criterion = nn.BCELoss()


optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)

# Training loop with progress tracking
num_epochs = 50
train_losses = []
val_losses = []

print("Starting training...")
for epoch in range(num_epochs):
    # Training phase
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0
    
    for batch_idx, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item() * inputs.size(0)
        
        # Calculate accuracy
        predicted = (torch.sigmoid(outputs) > 0.5).float()
        train_total += labels.size(0)
        train_correct += (predicted == labels).sum().item()
        
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.6f}")
    
    train_loss /= len(train_loader.dataset)
    train_acc = 100.0 * train_correct / train_total
    
    # Validation phase
    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * inputs.size(0)
            
            # Calculate accuracy
            predicted = (torch.sigmoid(outputs) > 0.5).float()
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()
    
    val_loss /= len(val_loader.dataset)
    val_acc = 100.0 * val_correct / val_total
    
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    
    scheduler.step(val_loss)
    
    print(f"Epoch {epoch+1}/{num_epochs}")
    print(f"Train Loss: {train_loss:.6f}, Train Acc: {train_acc:.2f}%")
    print(f"Val Loss: {val_loss:.6f}, Val Acc: {val_acc:.2f}%")
    print("-" * 50)
    
    # Save model periodically
    if (epoch + 1) % 10 == 0:
        save_model(model)

# Final model save
save_model(model)

# Plot training curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')

plt.subplot(1, 2, 2)
plt.plot(range(len(train_losses)), train_losses, label='Train')
plt.plot(range(len(val_losses)), val_losses, label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.yscale('log')
plt.title('Loss (Log Scale)')

plt.tight_layout()
plt.savefig(os.path.join(root_output, 'training_curves.png'), dpi=300, bbox_inches='tight')
plt.show()

print("Training completed!")
print(f"Final train loss: {train_losses[-1]:.6f}")
print(f"Final validation loss: {val_losses[-1]:.6f}")